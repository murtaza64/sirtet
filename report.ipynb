{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sir Tet Tetris AI Playground and Deep Q Learning agents for Tetris {-}\n",
    "\n",
    "The Sir Tet Playground is a simulation environment for Tetris AI agents designed for easy extensibility and modularity, where agents can be easily built, tested and benchmarked. Included are a few attempts at using Deep Q learning to play Tetris. The code for this project can be found at https://github.com/murtaza64/sirtet.\n",
    "\n",
    "# Introduction\n",
    "Tetris is a video game played on a 10x20 grid where a player must rotate and translate falling tetrominoes (contiguous pieces of four square blocks) to pack them efficiently, filling up horizontal lines of blocks to clear them. The objective of the game is to score as many points as possible before being unable to fit anymore blocks on the screen, where points are awarded for clearing lines, and clearing multiple lines with one block awards more points than separately clearing each of those lines.\n",
    "\n",
    "The simplified version of Tetris most of the existing AI literature examines allows the agent to select only a column and orientation of the tetromino to place, which entails that sliding falling pieces under a gap or performing 'T-spins' is not possible. However, this reduced version still captures a large portion of the game's difficulty: strategically placing tetrominoes in a way that enables future space-efficiency and clearing.\n",
    "\n",
    "# Related work\n",
    "Many approaches to Tetris agents have been taken over the past few decades. As the state space of Tetris is $2^{200} \\cdot 7\\cdot 7 \\approx 10^{62}$, much of the work in Tetris is about extracting features from states and/or state-action pairs, and selecting a good feature to reduce the state space, and then optimize over this reduced space. Dellacherie's hand-coded agent [1] uses a fixed set of six features of state-action pairs, and the agent simply chooses the action which maximizes the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{eroded piece cells} - \\text{col transitions} - \\text{row transitions} - \\text{landing height} - \\text{cumulative wells} - 4\\cdot\\text{holes}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of these features will appear in section 1.4.\n",
    "\n",
    "This hand-coded agent performs remarkably well, and most more sophisticated agents still struggle to match its performance. For learned optimizations, various methods have been attempted: successful genetic/evolutionary approaches [2, 3], some of which can beat Dellacherie's agent; several reinforcement learning approaches, including Least Square Policy Iteration [4] and Ant Colony Optimization [5], which generally fall short of the Dellacherie benchmark; and some novel approaches such as a feed-forward neural network with Particle Swarm Optimization [6] which achieves performance about half as good as Dellacherie. \n",
    "\n",
    "There are several existing literature reviews of the various approaches to Tetris collating performance results [7, 8]. However, one shortfall of these reviews and indeed the existing literature as a whole is that there doesn't seem to be a standard Tetris simulation envrionment, so slight differences in implementation, for example in approach to randomizing the next tetromino (grab-bag vs uniformly random), might be a source of slight inaccuracies in head-to-head comparisons of approaches. This is one of the motivations of introducing the Sir Tet playground, where agents with all manner of approach can be benchmarked against one another.\n",
    "\n",
    "There seems to be a lack of successful Q-learning approaches to Tetris in the literature, which was one of the motivations for trying linear and deep Q-learning approaches here. Additionally, Q-learning seemed like a good fit for a Tetris agent to be able to discover strategies for maximizing score without too much hand-coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sir Tet simulator and AI playground\n",
    "![image.png](sirtet_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sir Tet AI playground is implemented in Python with curses, so it can run in almost any terminal with color support, without the need for GUI functionality. It is designed for modularity and extensibility, with plug-and-play addition of agents which must only implement a `get_best_move` method, and the ability to watch agents play in real time as they train. In addition, you can play Tetris in your terminal.\n",
    "\n",
    "## Agent features\n",
    "Agents are implemented as python classes (inheriting from `BaseTetrisAgent`) and can be installed with a simple addition to `AIRunner` in `ai_runner.py`:\n",
    "```python\n",
    "class AIRunner(Runner):\n",
    "    \n",
    "    installed_agents : 'list[tuple[BaseTetrisAgent, set]]' = [\n",
    "        (TetrisQLearningAgent,  {Options.TRAINABLE}),\n",
    "        (DellacherieAgent,      {}),\n",
    "        (DeepQAgent,            {Options.TRAINABLE, Options.MONITOR, Options.SAVABLE}),\n",
    "        (DeepQExpReplayAgent,   {Options.TRAINABLE, Options.MONITOR, Options.SAVABLE})\n",
    "    ]\n",
    "```\n",
    "Agent initiailizers must take one positional argument which is a `logging.Logger` object.\n",
    "\n",
    "At minimum, an installed agent with a `get_best_move` method will support watching the agent play Tetris. While watching the agent, scores of individual games are printed out and a high score is tracked. The method should return a legal `(orientation, column)` integer pair.\n",
    "\n",
    "Agents with `Options.TRAINABLE` set must implement a `train(n_iters)` method which runs `n_iters` iterations of a training step. For the Q-learning approaches here, this iteration count corresponds to number of games to play, but other approaches could interpret this parameter in any other reasonable way. The training is done in a separate thread, while a (much slower) foreground thread displays a live view of the agent continuously playing games with its current training state, so the user can watch their agent learn and improve.\n",
    "\n",
    "Agents with `Options.SAVABLE` must implement `save(filename)` and `load(filename)` methods to save and load their state. For example, Keras-based agents can use `model.save_weights` and `model.load_weights`. When the user chooses to save or load weights, the checkpoint name they provide will be prepended with a directory named after the agent's class name before being sent to the `save` or `load` method (e.g. `DeepQAgent/checkpoint`).\n",
    "\n",
    "Finally, `Options.MONITOR` indicates that an agent has a `monitor()` method, which can be polled by the controller to retrieve arbitrary context about the current state of the agent for display to the user. The screenshot above shows an example of this, where the left panel contains information about hyperparameters and training statistics provided by the agent's `monitor()` method.\n",
    "\n",
    "## Recording demonstrations\n",
    "The Sir Tet playground lets you record demonstrations for agents that might require player data for pretraining steps (for example, Deep Q-Learning from Demonstrations). After entering the demonstration screen, you play Tetris for as long as you want, and when you choose to save them, a text file under `demonstrations/TIMESTAMP.demo` will be created. The format of of this text file consists of state-action pairs separated by newlines. The state is encoded by treating each row of the board as a 10-bit integer, concatenating the base 10 representations of the rows and appending the letter represenations of the current tetromino and next tetromino (from the set `zsjliot`). The action is encoded by an orientation index (for rotation of tetromino) and a column index. Here is an example of the demonstration syntax:\n",
    "```\n",
    "96/48/48/48/60/48/16/16/48/32/56/48/56/16/48/24/0/0/0/0|j|l:0,4\n",
    "```\n",
    "Helper functions for loading demonstrations from a file into memory are still in progress.\n",
    "\n",
    "## Sir Tet architecture\n",
    "### Game classes\n",
    "The `TetrisGameState` consists of a `TetrisBoard`, a current `Tetromino` and the next `Tetromino`.\n",
    "\n",
    "A `TetrisBoard` is internally represented as a list of lists, but can be queried by indexing it with a coordinate tuple (`board[x, y]`). The return value of this indexing operation is truthy if the cell is occupied and falsy if it is empty. Spaces outside the board are empty.\n",
    "\n",
    "`Tetromino`es can be indexed with an orientation index to receive an `OrientedTetromino`, which itself can be indexed relative to its bottom left cell with a coordinate tuple to check if that cell is occupied by the tetromino in its current orientation. For example, a `z` piece in orientation `0` will return `1` when indexed at `(1, 0)`, `(2, 0)`, `(0, 1)` or `(0, 2)`, and `0` at any other coordinate pair.\n",
    "\n",
    "Moves are represented by pairs of integers representing orientation index and column index.\n",
    "\n",
    "### Useful methods\n",
    "An agent may use the following methods to aid in making its decisions:\n",
    "\n",
    "`TetrisGameState.generate_move_context(orient, col)`: given a move, return `gameover, dummy, yi, cleared`. `gameover` is a bool which is `True` if the move resulted in a game over, and the other return values will be `None`. `dummy` is an instance of `TetrisBoard` with the move completed. It can be modified arbitrarily by the agent. `yi` is the column index at which the bottom left corner of the tetromino was placed. `cleared` is a list of row indices that were cleared by the move.\n",
    "\n",
    "`TetrisBoard.test_place_tetromino(tet : Tetromino, orient, col)`: given a tetromino and a move, return `dummy, yi, cleared` or raise `GameOver`. Used by `TetrisGameState.generate_move_context`.\n",
    "\n",
    "`TetrisBoard.score(cleared)`: given a list of cleared line indices, return the score rewarded.\n",
    "\n",
    "`TetrisGameState.get_moves()`: generator that yields all valid `(orient, col)` pairs. Note that moves that can cause a `GameOver` are still considered valid.\n",
    "\n",
    "Other methods of a `TetrisGameState` or its `TetrisBoard` should not be used by agents as they may mutate the controller's state.\n",
    "\n",
    "### Writing features\n",
    "Feature extractors of a state or state action pair may use the methods discussed above. Many features are implemented in `features.py` and `features2.py` (the latter of which has cleaner code). Here is an example feature:\n",
    "```python\n",
    "def holes(state : TetrisGameState, orient, col):\n",
    "    '''\n",
    "    count number of holes in board resulting from action (normalized to [0, 1])\n",
    "    '''\n",
    "    \n",
    "    #this preamble is standard for move-agnostic features\n",
    "    gameover, board, _, _ = state.generate_move_context(orient, col)\n",
    "    if gameover:\n",
    "        return 0\n",
    "\n",
    "    h = 0\n",
    "    for x, y in board.coords():\n",
    "        if not board[x, y]:\n",
    "            for ty in range(y + 1, HEIGHT):\n",
    "                if board[x, ty]:\n",
    "                    h += 1\n",
    "                    break\n",
    "    return h/200\n",
    "```\n",
    "\n",
    "### `Runner`s and `UserInterface`\n",
    "The Sir Tet playground consists of three `Runner`s found in `*_runner.py` which control the `UserInterface` object responsible for rendering the screen to the user. The latter has methods that make extending the platform more tractable, allowing easy displaying of text, prompting for strings, and manipulating the visible Tetris game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tetris Agents\n",
    "\n",
    "After the poor performance of the linear Q-learning agent from a few months ago, where there was difficulty in selecting a good feature set and tuning hyperparameters, I wanted to try the approach of taking the proven feature set of Dellacherie (that still outperforms many more complex approaches) and seeing if a non-linear, neural optimization function would achieve better results than the linear function.\n",
    "\n",
    "## Dellacherie's agent\n",
    "Dellacherie's agent, as described earlier, simply picks the move that optimizes the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{eroded piece cells} - \\text{col transitions} - \\text{row transitions} - \\text{landing height} - \\text{cumulative wells} - 4\\cdot\\text{holes}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these features are move-agnostic (i.e. features of the resulting board state only) while others are dependent on the move.\n",
    "\n",
    "`eroded_piece_cells`: number of lines cleared by move \\* number of bricks of placed tetromino cleared (e.g. if four lines are cleared by an `i`, this equals 16)\n",
    "\n",
    "`col_transitions`: number of vertical pairs of cells which contain both an empty cell and a block, i.e. number of vertical transitions from block to gap\n",
    "\n",
    "`row_transitions`: similar to `col_transitions`\n",
    "\n",
    "`landing_height`: height at which placed piece lands (=`yi` from `generate_move_context`)\n",
    "\n",
    "`cumulative_wells`: $\\sum_{w\\in wells} (1 + 2 + ... + depth(w))$ where a well is defined as a vertical sequence of empty cells with blocks on the left and right. \n",
    "\n",
    "`holes`: number of holes where a hole is defined as an empty cell with at least one block above it in the same column.\n",
    "\n",
    "The implementation of this agent was simple once the features were implemented:\n",
    "```python\n",
    "class DellacherieAgent(BaseTetrisAgent):\n",
    "\n",
    "    agent_name = 'Dellacherie\\'s legendary hand coded agent'\n",
    "\n",
    "    weights : 'dict[Callable[[TetrisGameState, int, int], float], float]' = {\n",
    "        eroded_piece_cells: 1.0,\n",
    "        col_transitions: -1.0,\n",
    "        row_transitions: -1.0,\n",
    "        landing_height: -1.0,\n",
    "        cumulative_wells: -1.0,\n",
    "        holes: -4.0\n",
    "    }\n",
    "\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def get_best_move(self, state: TetrisGameState) -> 'tuple[int, int]':\n",
    "        moves = list(state.get_moves())\n",
    "        move_scores = {(orient, col): sum(w*f(state, orient, col) for f, w in self.weights.items())\n",
    "            for orient, col in moves}\n",
    "        return max(moves, key=lambda m: move_scores[m])\n",
    "```\n",
    "\n",
    "## Deep Q-Learning (naive)\n",
    "This first approach to Deep Q-Learning uses a Q-value estimator network which takes the extracted features of a state-action pair as inputs and outputs a single Q value. The feature set was kept the same as Dellacherie's. The model consists of two fully connected hidden layers with 16 hidden units. There is also a target network to which the weights of the estimator are copied every 100 training steps. On each training step, the estimator chooses an action using the $\\epsilon$-greedy approach, and then calculates the loss\n",
    "\n",
    "$$l = (r_{s,a,s'} + max_{a'}\\hat{Q}(s', a') - Q(s, a))^2$$\n",
    "\n",
    "where $Q$ is the estimator model, $\\hat{Q}$ is the target model, and $r$ is the reward obtained in the transition resulting from the chosen action. This loss is used to update the weights of the estimator network using the `keras.optimizers.Adam` optimizer.\n",
    "\n",
    "## Deep Q-learning with Experience Replay\n",
    "After some reading, I realized that one common approach in Deep Q-learning is to randomly sample from past experiences to mitigate some of the correlation found in a standard sequential learning approach. This agent stores a buffer of observed state transitions (storing the input to the network instead of the actual state action pair), and at each training step samples a minibatch of experiences from the replay buffer, calculates the loss of the entire batch with respect to the current states of the estimator and target model, and performs optimization with these losses.\n",
    "\n",
    "## Results\n",
    "The Deep Q learning agents were trained for 8 hours each on approximately 3500 games. The experience replay model was trained with three different sets of hyperparameters, the best one being `copy_iterations=250` and `minibatch_size=64`.\n",
    "\n",
    "| Agent            | Average score (last 12 games) | High Score |\n",
    "| ---      | ---                   | ---        |\n",
    "| Dellacherie      | 41560                         | 194060     |\n",
    "| Deep Q Learning  | 365                           | 2940       |\n",
    "| DQL with ER      | 423                           | 3180       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and future work\n",
    "\n",
    "The results are surprisingly poor considering the feature set used was the same as Dellacherie's agent. I expected the networks to be able to at least match Dellacherie's performance by approximating a linear function of the features. \n",
    "\n",
    "One of the reasons for this poor performance might be insufficient training time. Even after eight hours of training, the average losses were in the hundreds. This might indicate that longer training times or larger batches of experiences should be used, or some of the learning rates need to be tweaked to obtain faster convergence. Alternatively, maybe the target network is still moving too fast for the estimator to converge to it. I was unable to test all of these factors due to time constraints.\n",
    "\n",
    "A possible improvement might be made by implementing Deep Q-Learning from demonstrations. Kick-starting the learning process with a set of user-recorded demonstrations might help the model by starting it off with a reasonable understanding of the strategy and allowing it to catch up to Dellacherie in performance. Unfortunately, because of time constraints, I was unable to implement the usage of recorded demonstrations in this project's timeframe.\n",
    "\n",
    "Of course, it's possible that the feature sets used in these models are the culprit--maybe Dellacherie's features are not suited to a complex non-linear optimizer. Maybe reducing the complexity of the network or trying different feature sets would have some positive impact on performance.\n",
    "\n",
    "Lastly, it's possible that Q-learning based approaches are simply infeasible for Tetris. This might be a result of the long sequences of actions without any rewards making it difficult to quickly get accurate Q-value estimates. Perhaps some form of reward shaping might combat this, but perhaps the reason the literature is light on Q-learning for Tetris is that it simply isn't a good solution for the problem.\n",
    "\n",
    "Hopefully whoever next tackles this problem will find the Sir Tet Playground and the agents I implemented useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Fahey, C. P. (2003). Tetris AI, Computer plays Tetris. http://colinfahey.com/tetris/tetris_en.html\n",
    "\n",
    "[2] Bohm, N., Kokai, G., and Mandl, S. (2005). An Evolutionary Approach to Tetris. *The Sixth Metaheuristics International Conference (MIC2005).*\n",
    "\n",
    "[3] Da Silva, R. S., and Parpinelli, R. S. (2017). Playing the Original Game Boy Tetris Using a Real\n",
    "Coded Genetic Algorithm. *2017 Brazilian Conference on Intelligent Systems.*\n",
    "\n",
    "[4] Lagoudakis, M. G., Parr, R., and Littman, M. L. (2002). Least-squares methods in reinforcement learning for control. *Hellenic Conference on Artificial Intelligence.*\n",
    "\n",
    "[5] Chen, X., Wang, H., Wang, W., Shi, Y., and Gao, Y. (2009). Apply ant colony optimization to tetris. *Proceedings of the 11th Annual conference on Genetic and evolutionary computation.*\n",
    "\n",
    "[6] Langenhoven, L., Van Heerden, W. S., and Engelbrecht, A. P. (2010). Swarm\n",
    "tetris: Applying particle swarm optimization to tetris. *Evolutionary\n",
    "Computation (CEC), 2010 IEEE Congress on.*\n",
    "\n",
    "[7] Thiery, C., Scherrer, B. (2009). Building Controllers for Tetris. *International Computer Games\n",
    "Association Journal*\n",
    "\n",
    "[8] Carr, D. (2005). Applying reinforcement learning to Tetris. *Rhodes University*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
